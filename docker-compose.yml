
networks:
  custom_network:
    name: EcoEventSys
    driver: bridge

services:
  # PostgreSQL Instance Service
  postgres:
    image: docker.io/bitnami/postgresql:17
    networks:
      - custom_network
    container_name: Container_PostgreSQL
    ports:
      - "5432:5432"
    volumes:
      - ./source-layer/setup.sql:/docker-entrypoint-initdb.d/setup.sql
    environment:
      - POSTGRES_USER=analytics_user
      - POSTGRES_PASSWORD=myStrongPassword123
      - POSTGRES_DB=streaming_db

  # Kakfa service
  kafka:
    image: "docker.io/bitnami/kafka:3.9.0"
    restart: "no"
    networks:
      - custom_network
    container_name: Container_Kafka
    ports:
      - "9092:9092" # this for external access
      - "29092:29092" # or internal Docker service communication
      - "9093:9093" # Controller communication
    volumes:
      - "kafka_data:/bitnami"
    environment:
      - KAFKA_CFG_NODE_ID=0
      - KAFKA_CFG_PROCESS_ROLES=controller, broker
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,PLAINTEXT_INTERNAL://:29092,CONTROLLER://:9093
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092,PLAINTEXT_INTERNAL://kafka:29092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:9093
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR=1
      - KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1
      # - KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR=1
      # - KAFKA_CFG_GROUP_INITIAL_REBALANCE_DELAY_MS=0

  # This service named kafdrop is for monitoring kafka cluster
  kafdrop:
    image: docker.io/obsidiandynamics/kafdrop:4.1.0
    restart: "no"
    networks:
      - custom_network
    container_name: Container_Kafdrop
    ports:
      - "9000:9000"
    environment:
      - KAFKA_BROKERCONNECT=kafka:29092
    depends_on:
      - kafka
    volumes:
      - "kafdrop_data:/obsidiandynamics"
  
  # Spark Master Service
  spark-master:
    image: docker.io/bitnami/spark:3.5.1
    networks:
      - custom_network
    container_name: Container-Spark-Master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    ports:
      - "8080:8080"
      - "7077:7077"
    depends_on:
      - kafka
    volumes:
      - "spark-master_data:/bitnami"

  # Spark Worker 0, we could add a second worker 1 if necessary
  spark-worker:
    image: docker.io/bitnami/spark:3.5.1
    networks:
      - custom_network
    container_name: Container-Spark-Worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=4G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    ports:
      - "8081:8081"
    depends_on:
      - spark-master
    volumes:
      - "spark-worker_data:/bitnami"
  
  # Redis Service for caching
  redis:
    image: docker.io/redis/redis-stack:latest
    networks:
      - custom_network
    container_name: Container_Redis
    restart: "unless-stopped"
    # The Redis service is configured to allow empty passwords for development purposes.
    # In production, it is recommended to set a strong password.
    environment:
      # ALLOW_EMPTY_PASSWORD is recommended only for development.
      - ALLOW_EMPTY_PASSWORD=yes
      - REDIS_DISABLE_COMMANDS=FLUSHDB,FLUSHALL
    ports:
      - "6379:6379"
      - "8001:8001" # Optional, for Redis GUI like RedisInsight
    volumes:
      - "redis_data:/bitnami"

  # Cassandra Service
  cassandra:
    image: docker.io/bitnami/cassandra:latest
    # build:
    #   context: ./serving-layer/cassandra
    #   dockerfile: Dockerfile
    restart: "unless-stopped"
    networks:
      - custom_network
    container_name: Container_Cassandra
    ports:
      - "9042:9042"
    environment:
      - CASSANDRA_CLUSTER_NAME=EcoEventSysCluster
      - CASSANDRA_DC=EcoEventSysDC
      - CASSANDRA_RACK=EcoEventSysRack
      # Set your desired username/password
      - CASSANDRA_USER=cassandra
      - CASSANDRA_PASSWORD=cassandra
    volumes:
      - "cassandra_data:/var/lib/cassandra/data"
  # This service is to initialize the Cassandra with our keyspace and table
  cassandra-init:
    image: cassandra:4.1
    networks:
      - custom_network
    restart: "no"
    container_name: Cassandra-Setup-Container
    depends_on:
      - cassandra
    entrypoint: ["/bin/sh", "-c"]
    command: |
      "
      until cqlsh cassandra -u cassandra -p cassandra -e 'DESCRIBE KEYSPACES'; do
        echo 'Waiting for Cassandra...';
        sleep 5;
      done &&
      cqlsh cassandra -u cassandra -p cassandra -f /cassandra-setup.cql
      cqlsh cassandra -u cassandra -p cassandra -e 'DESCRIBE KEYSPACES'
      "
    volumes:
      - ./serving-layer/cassandra/cassandra-setup.cql:/cassandra-setup.cql
    # Service that create kafka topic <<engagement_events>> for our use case
  topic-creation:
    networks:
      - custom_network
    restart: "no"
    build:
      context: ./message-borker-layer
      dockerfile: Dockerfile
    container_name: Topics-Setup-Container
    depends_on:
      - kafka
  
  # The Ingestor Service
  ingestion:
    networks:
      - custom_network
    restart: "no"
    build:
      context: ./ingestion-layer
      dockerfile: Dockerfile
    container_name: Ingestion-Container
    depends_on:
      - kafka
      - postgres
    env_file:
      - ./ingestion-layer/.env
    ports:
      - "5000:5000"
  
  # The Processing Service
  processing:
    networks:
      - custom_network
    restart: "no"
    build:
      context: ./processing-layer
      dockerfile: Dockerfile
    container_name: Processing-Container
    depends_on:
      - spark-master
      - spark-worker
      - kafka
      - postgres
      - cassandra
      - redis
    # env_file:
    #   - ./processing-layer/.env
    ports:
      - "4040:4040" # Spark UI port for monitoring
    volumes:
      - "processing-layer_data:/app"

volumes:
  postgresql_data:
    driver: local
  kafka_data:
    driver: local
  kafdrop_data:
    driver: local
  spark-master_data:
    driver: local
  spark-worker_data:
    driver: local
  redis_data:
    driver: local
  cassandra_data:
    driver: local
  processing-layer_data:
    driver: local