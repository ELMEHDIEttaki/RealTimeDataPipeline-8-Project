
networks:
  custom_network:
    name: EcoEventSys
    driver: bridge

services:
  # PostgreSQL Instance Service
  postgresql:
    image: docker.io/bitnami/postgresql:17
    networks:
      - custom_network
    container_name: Container_PostgreSQL
    ports:
      - '5432:5432'
    volumes:
      - ./source-layer/setup.sql:/docker-entrypoint-initdb.d/setup.sql
    environment:
      - POSTGRES_USER=analytics_user
      - POSTGRES_PASSWORD=myStrongPassword123
      - POSTGRES_DB=streaming_db


  # Kakfa service
  kafka:
    image: "docker.io/bitnami/kafka:3.9.0"
    restart: "no"
    networks:
      - custom_network
    container_name: Container_Kafka
    ports:
      - "9092:9092" # this for external access
      - "29092:29092" # or internal Docker service communication
      - "9093:9093" # Controller communication
    volumes:
      - "kafka_data:/bitnami"
    environment:
      - KAFKA_CFG_NODE_ID=0
      - KAFKA_CFG_PROCESS_ROLES=controller, broker
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,PLAINTEXT_INTERNAL://:29092,CONTROLLER://:9093
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092,PLAINTEXT_INTERNAL://kafka:29092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:9093
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR=1
      - KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1
      # - KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR=1
      # - KAFKA_CFG_GROUP_INITIAL_REBALANCE_DELAY_MS=0

  # This service named kafdrop is for monitoring kafka cluster
  kafdrop:
    image: docker.io/obsidiandynamics/kafdrop:4.1.0
    restart: "no"
    networks:
      - custom_network
    container_name: Container_Kafdrop
    ports:
      - "9000:9000"
    environment:
      - KAFKA_BROKERCONNECT=kafka:29092
    depends_on:
      - kafka
    volumes:
      - "kafdrop_data:/obsidiandynamics"
  
  # Spark Master Service
  spark-master:
    image: docker.io/bitnami/spark:3.5.1
    networks:
      - custom_network
    container_name: Container-Spark-Master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    ports:
      - "8080:8080"
      - "7077:7077"
    depends_on:
      - kafka
    volumes:
      - "spark-master_data:/bitnami"

  # Spark Worker 0, we could add a second worker 1 if necessary
  spark-worker:
    image: docker.io/bitnami/spark:3.5.1
    networks:
      - custom_network
    container_name: Container-Spark-Worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_MEMORY=4G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    ports:
      - "8081:8081"
    depends_on:
      - spark-master
    volumes:
      - "spark-worker_data:/bitnami"
  
    # Service that create kafka topic <<engagement_events>> for our use case
  topic-creation:
    networks:
      - custom_network
    restart: "no"
    build:
      context: ./message-borker-layer
      dockerfile: Dockerfile
    container_name: Topics-Setup-Container
    depends_on:
      - kafka

    # Service create tables on postgresql database
  # tables-creation:
  #   networks:
  #     - custom_network
  #   restart: "no"
  #   build:
  #     context: ./source-layer
  #     dockerfile: Dockerfile
  #   container_name: Tables-Setup-Container
  #   depends_on:
  #     - postgresql
  #   environment:
  #     - DB_HOST=postgresql
  #     - DB_USER=analytics_user
  #     - DB_NAME=streaming_db
  #     - POSTGRES_PASSWORD=myStrongPassword123

# secrets:
#   db_password:
#     file: ./secrets/db_password.txt

volumes:
  postgresql_data:
    driver: local
  kafka_data:
    driver: local
  kafdrop_data:
    driver: local
  spark-master_data:
    driver: local
  spark-worker_data:
    driver: local